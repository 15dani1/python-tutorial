{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ISRC Python Workshop 2: Web Scraping (Mar. 09th 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use off-the-shelf tools: _Do not reinvent the wheel!!!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Application Programming Interfaces (APIs): Twitter API as an example\n",
    "\n",
    "Many web servers have their own APIs ready to use. By using these convenient tools, we can get started right off following their documentations and examples without any manual efforts. We will be using <a href=\"https://apps.twitter.com/\" target=\"_blank\">Twitter API</a> as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to register an account for Twitter Developer and register an app. Let's go to https://dev.twitter.com/ and get an app togther. <a href=\"https://python-twitter.readthedocs.io/en/latest/getting_started.html\" target=\"_b lank\">Here</a>'s a quick start on how you can do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we obtain *__consumer key__*, *__consumer secret__*, *__access token__*, and *__access token secret__*, we are ready to retrieve some data from Twitter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## read my app keys\n",
    "with open(\"./twitter_keys.csv\", \"r\") as twitter_keys:\n",
    "    keys = twitter_keys.read()\n",
    "    consumer_key, consumer_secret, access_token, access_secret = \\\n",
    "        keys.split(\"\\n\")[:-1]\n",
    "\n",
    "## load twitter package, which a well-written Python package for Twitter APIs\n",
    "import twitter\n",
    "api = twitter.Api(consumer_key=consumer_key,\n",
    "                  consumer_secret=consumer_secret,                  \n",
    "                  access_token_key=access_token,\n",
    "                  access_token_secret=access_secret)\n",
    "\n",
    "## check status\n",
    "print(api.VerifyCredentials())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Try to do some simple tasks\n",
    "# get statuses\n",
    "statuses = api.GetUserTimeline(screen_name=\"Zhiya Zuo\")\n",
    "print([s.text for s in statuses])\n",
    "statuses = api.GetUserTimeline(user_id=\"2740697738\")\n",
    "print([s.created_at for s in statuses])\n",
    "\n",
    "## Get your friends\n",
    "friends = api.GetFriends()\n",
    "print([f.name for f in friends])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## More interestingly, let's go get some tweets from Twitter\n",
    "## See https://dev.twitter.com/rest/public/search for more informaiton on how to construct a query\n",
    "results = api.GetSearch(\n",
    "    raw_query=\"q=uiowa&result_type=popular&since=2014-12-01&count=20&lang=en\")\n",
    "# How to set `lang` parameter -> https://dev.twitter.com/rest/reference/get/help/languages\n",
    "\n",
    "# show all the text in the retrieved tweets, with user screen name highlited\n",
    "from IPython.display import clear_output\n",
    "for tw in results:\n",
    "    clear_output() # clear output in iPython Notebook after each print\n",
    "    print(\"%s tweeted by \\033[41m%s\\033[0m\"%(tw.text, tw.user.screen_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create your own manual parsing programs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually scraping\n",
    "\n",
    "Sometimes, they have APIs but they have no well-written packages in the language you prefer (e.g. only Java but no Python libraries). Even worse, there may not be APIs for the public and we have to design a scraper to retrieve all the relevant informaiton we want. In such cases, we can manually look at the returned values and build our own wrapper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Preliminiary examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Examples from <a href=\"https://www.w3schools.com/html/tryit.asp?filename=tryhtml_basic_document\" target=\"blank_\">w3schools</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<h1>My First Heading</h1>\n",
    "\n",
    "<p>My 1st paragraph.</p>\n",
    "<p>My 2nd paragraph.</p>\n",
    "<p>My 3rd paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save this code to your disk as `sample.html` (or any other name). We will use a great library called ___`Beautiful Soup`___ to read the contents from Python. You may also need to install lxml, which is for parsing specific formats (e.g., html and xml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Do the following if you have not\n",
    "# pip install beautifulsoup4 lxml\n",
    "\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "with open(\"./sample.html\", \"r\") as sample:\n",
    "    sample_contents = sample.read()\n",
    "\n",
    "sample_soup = Soup(sample_contents)\n",
    "# by printing it, we can see the exact contents as shown above with proper indentation\n",
    "print(sample_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Get the contents of interest: all the p's\n",
    "# ** p means paragraph in html. Check more tag definitions on w3schools.org\n",
    "p_tags = sample_soup.find_all(\"p\")\n",
    "for p in p_tags:\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locate your contents of interest in browser\n",
    "As you can see, this is very straightforward. Let's use a real website for illustration. For example, if we are interested in company profiles, we can scrape from Google Finance. We will be using <a href=\"https://www.google.com/finance?q=NYSE%3AIBM&ei=Ij62WPHgGdSLmAHrja_wAQ\" target=\"_blank\">IBM's profile</a> as an example. However, you may find off-the-shelf packages. We will only use this for an introduction on how to scrape manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the \"text style\" or the real structure of a web page, you can use ___`developer tools`___ function in your browser. For example, you can see something like this. If you move your mouse to a place, the console will show you the corresponding tags in the source html files. You will find that the description text is located within a `p` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/IEl2uyG.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib2 \n",
    "ibm_url = \"https://www.google.com/finance?q=NYSE%3AIBM&ei=Ij62WPHgGdSLmAHrja_wAQ\"\n",
    "## read it as strings\n",
    "ibm_page = urllib2.urlopen(ibm_url).read()\n",
    "## convert it to a soup object\n",
    "ibm_soup = Soup(ibm_page)\n",
    "## find the correponding tag. Note that class_ has a trailing underscore\n",
    "summary_tag = ibm_soup.find(\"div\", class_=\"companySummary\")\n",
    "print(\"\\033[43m%s\\033[0m\"%(summary_tag.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, you can scrape almost any webpage of interest. Other formats such as <a href=\"http://www.json.org/\" target=\"_blank\">JSON</a> and <a href=\"https://www.w3.org/XML/\" target=\"_blank\">XML</a> do have high similarities and a few differences. They are not very difficult to know the basics!\n",
    "\n",
    "***But keep in mind that you should act politely, with propoer permission. To find out whether specific paths/contents are allowed to be scraped, you can check their ___`robots.txt`___. For example, <a href=\"https://www.google.com/robots.txt\" target=\"_blank\">here's</a> the permission information set by Google.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exercise: build our own Scopus scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using <a href=\"https://dev.elsevier.com/\" target=\"_blank\">Scopus APIs</a> as an example (it seems like they are actively building such tools to facilitate developing applications but we can still DIY wrappers that most fit our own usage). I build my own called <a href=\"http://zhiyzuo.github.io/python-scopus/\" target=\"_blank\">python-scopus</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will still be getting your API key for access to Elsevier's database. Go to https://dev.elsevier.com/ and request a key. Then we can write a simple query to try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"./scopus_apikey.csv\", \"r\") as scopus_key:\n",
    "    key = scopus_key.read().strip(\"\\n\")\n",
    "## check out http://api.elsevier.com/documentation/SCOPUSSearchAPI.wadl\n",
    "## search tips: http://api.elsevier.com/documentation/search/SCOPUSSearchTips.htm\n",
    "search_uri = \"http://api.elsevier.com/content/search/scopus\"\n",
    "\n",
    "import requests\n",
    "## notice how we supply the query parameters using requests package\n",
    "## http://docs.python-requests.org/en/master/user/quickstart/#make-a-request\n",
    "par = {'apikey': key, 'query': 'TITLE-ABS-KEY(protein structure)'}\n",
    "r = requests.get(search_uri, params=par)\n",
    "## click to see\n",
    "print(\"XML: %s\"%r.url)\n",
    "\n",
    "## by default this is an XML repsonse, we can ask elsevier to return a JSON object\n",
    "par_json = {'apikey': key, 'query': 'TITLE-ABS-KEY(protein structure)', 'httpAccept':'application/json'}\n",
    "r_json = requests.get(search_uri, params=par_json)\n",
    "## click to see\n",
    "print(\"JSON: %s\"%r_json.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parsing the JSON object, you can use <a href=\"http://docs.python-guide.org/en/latest/scenarios/json/\" target=\"_blank\">***`JSON`***</a> library, which we will cover later. \n",
    "\n",
    "So, we currently have something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i.imgur.com/4NnM00t.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually use the same trick as we did for the html files for xml formats: first download it using ___`urlopen`___ function in ___`urllib2`___ library. However, ___`requests`___ make all these much easier for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## a method to convert the response into JSON!\n",
    "resp = r.json()\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## by default it will return 25 results.\n",
    "print(\"Showing %s items.\"%len(resp['search-results']['entry']))\n",
    "## use start index to iterate thru all results\n",
    "total_count = resp['search-results']['opensearch:totalResults']\n",
    "start_index = resp['search-results']['opensearch:startIndex']\n",
    "print(\"A total number of %s results found in Scopus; Start index is %s.\"%(total_count, start_index))\n",
    "## Use a dictionary-style to access the values.\n",
    "print(\"\\033[43m%s\\033[0m\"%resp['search-results']['entry'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we only want the titles, we can write a very simple function to build a wrapper for code reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_scopus(key, query, index=0, verbose=True):\n",
    "    '''\n",
    "        Search Scopus database using key as api key, with query.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        key : string\n",
    "            Elsevier api key. Get it here: https://dev.elsevier.com/index.html\n",
    "        query : string\n",
    "            Search query. See more details here: http://api.elsevier.com/documentation/search/SCOPUSSearchTips.htm\n",
    "        index : int\n",
    "            Start index. Will be used in search_scopus_plus function\n",
    "        verbose : bool\n",
    "            Verbose mode.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            id column stores scopus id and title column stores titles\n",
    "    '''\n",
    "    \n",
    "    import requests\n",
    "    par = {'apikey': key, 'query': 'TITLE-ABS-KEY(protein structure)', 'start': index}\n",
    "    r = requests.get(search_uri, params=par)\n",
    "    resp = r.json()\n",
    "    ## print out some summaries\n",
    "    total_count = resp['search-results']['opensearch:totalResults']\n",
    "    start_index = resp['search-results']['opensearch:startIndex']\n",
    "    entries = resp['search-results']['entry']\n",
    "    if verbose:\n",
    "        print(\"Going to: %s\"%r.url)\n",
    "        print(\"A total number of %s results found in Scopus; Showing %d; Start index is %s.\"%\\\n",
    "              (total_count, len(entries), start_index))\n",
    "        print(\"--------------------------------------\")  \n",
    "\n",
    "    # iterate thru each entry\n",
    "    # You can also do this in a list comprehension with one line of code\n",
    "    # (I did not test the following code but it should work with minimal modification)\n",
    "    # id_title_list = [(entry['dc:identifier'].split(\":\")[-1], entry[\"dc:title\"]) for entry in entries]\n",
    "    id_title_list = []\n",
    "    for entry in entries:\n",
    "        scopus_id = entry['dc:identifier'].split(\":\")[-1]\n",
    "        id_title_list.append((scopus_id, entry[\"dc:title\"]))\n",
    "    \n",
    "    ## use pd.DataFrame to better store the results\n",
    "    import pandas as pd\n",
    "    result_df = pd.DataFrame(id_title_list, columns=list((\"id\", \"title\")))\n",
    "    return(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df = search_scopus(key, 'TITLE-ABS-KEY(protein structure)')\n",
    "# use iPython's functions to print data frames prettier\n",
    "from IPython.display import display, HTML\n",
    "display(result_df)\n",
    "# OR: HTML(result_df.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Conclusion\n",
    "\n",
    "Hopefully you see how the two different types of web data retrieval work and you will be able to scrape your own data now! Note that in our last example of scraping Scopus data, we only get 25 results. By a simple tweak of varying the indices, we can retrieve all of them! The following is a revised version of ***`search_scopus`*** to get all entries, just for your references, which is part of my code for ***`python-scopus`***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_scopus_plus(key, query, number=200):\n",
    "    '''\n",
    "        Search Scopus database using key as api key, with query.\n",
    "        Reuse function ssearch_scopus(key, query, index, verbose)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        key : string\n",
    "            Elsevier api key. Get it here: https://dev.elsevier.com/index.html\n",
    "        query : string\n",
    "            Search query. See more details here: http://api.elsevier.com/documentation/search/SCOPUSSearchTips.htm\n",
    "        number : int\n",
    "            The number of entries to return. By default it is 200.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pandas DataFrame\n",
    "            id column stores scopus id and title column stores titles\n",
    "    '''\n",
    "    \n",
    "    if type(number) is not int or number > total_count:\n",
    "        import sys\n",
    "        raise ValueError(\"%s is not a valid input for the number of entries to return.\" %number)\n",
    "    \n",
    "    results_df = search_scopus(key, query, verbose=False)\n",
    "    if number < 25:\n",
    "        # if less than 25, just one page of response is enough\n",
    "        return results_df[:number]\n",
    "    \n",
    "    # if larger than, go to next few pages until enough\n",
    "    index = 1\n",
    "    while True:\n",
    "        results_df = results_df.append(search_scopus(key, query, index, False), ignore_index=True)\n",
    "        if len(results_df) >= number:\n",
    "            return results_df[:number]\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df_20 = search_scopus_plus(key, 'TITLE-ABS-KEY(protein structure)',20)\n",
    "display(result_df_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_df_40 = search_scopus_plus(key, 'TITLE-ABS-KEY(protein structure)',40)\n",
    "display(result_df_40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
